odule assessment
Completed
200 XP
10 Minutes
Great job! You passed the module assessment.
Module assessment passed

Great job! You passed the module assessment. Sign in to save progress.
Score: 100%

AI-generated content The questions and answer choices in this module assessment were generated using AI and reviewed by a human author.


1. Which setting in Microsoft Fabric allows you to specify default resource files that need to be available in a custom environment? 
Upload resource files directly in the environment settings.
Correct
Select memory optimized nodes in the node family selection.
Enable dynamic allocation in the Spark pool settings.
2. Which SQL magic command should you use in a Microsoft Fabric notebook to execute SQL queries directly? 
%%sql
Correct
%%spark
%%pyspark
3. You need to calculate the average sales amount for each product category in a Spark dataframe. Which code snippet accurately achieves this task? 
avg_sales_df = df.groupBy('Category').agg({'SalesAmount': 'avg'})
Correct
avg_sales_df = df.select('Category', 'avg(SalesAmount)')
avg_sales_df = df.agg({'SalesAmount': 'mean'}).groupBy('Category')
4. A data scientist wants to compare product performance across different categories using Spark SQL in Microsoft Fabric. What is an efficient approach to accomplish this task? 
Use Spark SQL to perform group-by queries on product data and visualize results using notebook charts.
Correct
Use Excel for data comparison, then import results into a notebook for visualization.
Analyze product data using Python scripts outside Microsoft Fabric.
5. Which outcome can be expected by configuring a Spark pool with dynamic allocation enabled in Microsoft Fabric? 
CPU usage is minimized by restricting node operations.
Executor processes are allocated based on data volume, optimizing resource usage.
Correct
Spark jobs will always use the maximum number of nodes available.
6. What is the expected result of executing the following PySpark command: df.groupBy('Category').agg({'SalesAmount': 'sum'})? 
A dataframe with each category and the average sales amounts for each category.
A new dataframe with each category and the sum of sales amounts for each category.
Correct
A list of categories with their corresponding total sales amounts.
7. Which Spark Dataframe method is most efficient for finding the maximum value in a column of numerical data? 
filter
groupBy
agg
Correct
8. You have created a dataframe from a CSV file and want to save it as a Delta table in the Spark catalog for future SQL queries. Which method should you use? 
df.createOrReplaceTempView("view_name")
df.write.format("delta").saveAsTable("table_name")
Correct
df.write.mode("overwrite").parquet("path")
9. What configuration should be set in a custom environment to ensure that specific Python libraries are available for a Spark job in Microsoft Fabric? 
Configure the Spark runtime to the latest version available.
Install specific public libraries from the Python Package Index (PyPI) in the environment settings.
Correct
Select the high concurrency mode in the Spark settings.